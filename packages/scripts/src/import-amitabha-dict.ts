/**
 * é˜¿å½Œé™€ä½›è–æ•™å¤§è¾­å…¸å¯¼å…¥è„šæœ¬
 *
 * è¿™æ˜¯ä¸€ä¸ªç»¼åˆè¯å…¸ï¼ŒåŒ…å«20éƒ¨ä½›å­¦è¯å…¸ï¼Œéœ€è¦æŒ‰æ¥æºåˆ†åˆ«å¯¼å…¥
 * å·²å­˜åœ¨çš„è¯å…¸ï¼ˆä½›å…‰å¤§è¾å…¸ã€ä¸ç¦ä¿ä½›å­¦å¤§è¾å…¸ï¼‰ä¼šè·³è¿‡
 *
 * ä½¿ç”¨æ–¹æ³•ï¼š
 *   pnpm tsx src/import-amitabha-dict.ts <mdxæ–‡ä»¶è·¯å¾„>
 */

import pg from 'pg'
import { toSimplified } from './zhconv.js'
import { execSync } from 'child_process'

// æ•°æ®åº“è¿æ¥
const pool = new pg.Pool({
  database: 'cbeta',
  host: '/var/run/postgresql',
})

// æ¥æºæ˜ å°„ï¼šç¹ä½“ -> ç®€ä½“æ ‡å‡†å
const SOURCE_MAP: Record<string, string> = {
  'ä½›å…‰å¤§è¾­å…¸å¢è¨‚ç‰ˆ': 'ä½›å…‰å¤§è¾å…¸',
  'ä¸ç¦ä¿ä½›å­¸å¤§è¾­å…¸': 'ä¸ç¦ä¿ä½›å­¦å¤§è¾å…¸',
  'ä½›æ•™å“²å­¸å¤§è¾­å…¸(æ—¥è“®)': 'ä½›æ•™å“²å­¦å¤§è¾å…¸(æ—¥è²)',
  'ä½›æ•™äººç‰©å‚³': 'ä½›æ•™äººç‰©ä¼ ',
  'é‡‹æ°å…­å¸–': 'é‡Šæ°å…­å¸–',
  'ä¸­è¯ä½›æ•™ç™¾ç§‘å…¨æ›¸': 'ä¸­åä½›æ•™ç™¾ç§‘å…¨ä¹¦',
  'ä½›å­¸å¸¸è¦‹è¾­å½™': 'ä½›å­¦å¸¸è§è¾æ±‡',
  'æ–°ç·¨ä½›æ•™è¾­å…¸(é™³å…µ)': 'æ–°ç¼–ä½›æ•™è¾å…¸(é™ˆå…µ)',
  'ä¸­åœ‹ä½›å­¸äººåè¾­å…¸(æ˜å¾©æ³•å¸«)': 'ä¸­å›½ä½›å­¦äººåè¾å…¸',
  'å—å±±å¾‹å­¸è¾­å…¸': 'å—å±±å¾‹å­¦è¾å…¸',
  'é˜¿æ¯—é”ç£¨è¾­å…¸': 'é˜¿æ¯—è¾¾ç£¨è¾å…¸',
  'ä¿—èªä½›æº': 'ä¿—è¯­ä½›æº',
  'äº”ç™¾ç¾…æ¼¢': 'äº”ç™¾ç½—æ±‰',
  'ä¸­åœ‹ç™¾ç§‘å…¨æ›¸ä½›æ•™ç¯‡': 'ä¸­å›½ç™¾ç§‘å…¨ä¹¦ä½›æ•™ç¯‡',
  'ä¸­åœ‹ä½›æ•™å°ç™¾ç§‘': 'ä¸­å›½ä½›æ•™å°ç™¾ç§‘',
  'ä½›æ•™å¸¸ç”¨å”„å™¨å™¨ç‰©æœè£ç°¡è¿°': 'ä½›æ•™å¸¸ç”¨å‘—å™¨å™¨ç‰©æœè£…ç®€è¿°',
  'æ³•è‹‘è«‡å¢(å‘¨å”è¿¦)': 'æ³•è‹‘è°ˆä¸›(å‘¨å”è¿¦)',
  'ä½›æºèªè©è©å…¸': 'ä½›æºè¯­è¯è¯å…¸',
}

// å·²å­˜åœ¨çš„è¯å…¸ï¼ˆè·³è¿‡å¯¼å…¥ï¼‰
const EXISTING_SOURCES = new Set(['ä½›å…‰å¤§è¾å…¸', 'ä¸ç¦ä¿ä½›å­¦å¤§è¾å…¸'])

// è¯å…¸æ¥æºè¯†åˆ«å…³é”®è¯
const DICT_KEYWORDS = ['è¾­å…¸', 'è¾­å½™', 'ç™¾ç§‘', 'äººç‰©å‚³', 'ç¾…æ¼¢', 'ä½›æº', 'å…­å¸–', 'ä¿—èª', 'æ³•è‹‘', 'å™¨ç‰©', 'å¾‹å­¸', 'å› æ˜']

interface DictEntry {
  term: string
  termSimplified: string
  definition: string
  definitionText: string
  source: string
}

/**
 * ä½¿ç”¨ Python readmdict è§£æ MDX æ–‡ä»¶
 */
async function parseMdxWithPython(mdxPath: string): Promise<DictEntry[]> {
  console.log(`\nğŸ“– æ­£åœ¨è§£æ MDX æ–‡ä»¶...`)

  const pythonScript = `
import sys
import json
from readmdict import MDX
import re

mdx_path = sys.argv[1]
mdx = MDX(mdx_path)
items = list(mdx.items())

source_pattern = r'ã€([^ã€‘]+)ã€‘'
dict_keywords = ${JSON.stringify(DICT_KEYWORDS)}

entries = []
for key, value in items:
    if isinstance(key, bytes):
        key = key.decode('utf-8')
    if isinstance(value, bytes):
        value = value.decode('utf-8')

    # æå–æ¥æº
    match = re.search(source_pattern, value)
    if not match:
        continue

    source = match.group(1)

    # æ£€æŸ¥æ˜¯å¦æ˜¯è¯å…¸æ¥æº
    is_dict = False
    for kw in dict_keywords:
        if kw in source:
            is_dict = True
            break

    if not is_dict:
        continue

    # æ¸…ç†è¯æ¡åï¼ˆå»é™¤å‰å¯¼æ•°å­—åºå·ï¼‰
    term = key.strip()
    if re.match(r'^\\d+\\s*', term):
        term = re.sub(r'^\\d+\\s*', '', term)

    # è·³è¿‡ç©ºè¯æ¡æˆ–å¤ªçŸ­çš„è¯æ¡
    if len(term) < 1:
        continue

    # è·³è¿‡ç›®å½•ç±»è¯æ¡
    if 'ç¸½ç›®éŒ„' in term or 'è£½ä½œèªªæ˜' in term or 'åˆ¶ä½œèªªæ˜' in term:
        continue

    entries.append({
        'term': term,
        'definition': value,
        'source': source
    })

# è¾“å‡ºä¸º JSON Lines æ ¼å¼
for entry in entries:
    print(json.dumps(entry, ensure_ascii=False))
`

  // å†™å…¥ä¸´æ—¶ Python è„šæœ¬
  const fs = await import('fs')
  const path = await import('path')
  const tempScript = '/tmp/parse_mdx.py'
  fs.writeFileSync(tempScript, pythonScript)

  // è¿è¡Œ Python è„šæœ¬
  console.log(`   æ­£åœ¨è§£æè¯æ¡...`)
  const result = execSync(
    `source /home/guang/happy/yoho-cbeta/.venv/bin/activate && python3 ${tempScript} "${mdxPath}"`,
    {
      maxBuffer: 500 * 1024 * 1024, // 500MB buffer
      shell: '/bin/bash',
    }
  ).toString()

  const lines = result.trim().split('\n').filter(Boolean)
  console.log(`   è§£æåˆ° ${lines.length} æ¡è¯æ¡`)

  const entries: DictEntry[] = []
  for (const line of lines) {
    try {
      const data = JSON.parse(line)
      const sourceName = SOURCE_MAP[data.source] || toSimplified(data.source)

      entries.push({
        term: data.term,
        termSimplified: toSimplified(data.term),
        definition: data.definition,
        definitionText: extractText(data.definition),
        source: sourceName,
      })
    } catch {
      // è·³è¿‡è§£æå¤±è´¥çš„è¡Œ
    }
  }

  return entries
}

/**
 * æ¸…ç† HTML å¹¶æå–çº¯æ–‡æœ¬
 */
function extractText(html: string): string {
  return sanitizeString(html)
    .replace(/<[^>]+>/g, '')
    .replace(/&nbsp;/g, ' ')
    .replace(/&lt;/g, '<')
    .replace(/&gt;/g, '>')
    .replace(/&amp;/g, '&')
    .replace(/&quot;/g, '"')
    .replace(/&#(\d+);/g, (_, code) => String.fromCharCode(parseInt(code)))
    .replace(/\s+/g, ' ')
    .trim()
}

/**
 * æ¸…ç†æ— æ•ˆå­—ç¬¦
 */
function sanitizeString(str: string): string {
  return str.replace(/\x00/g, '').replace(/[\x01-\x08\x0B\x0C\x0E-\x1F]/g, '')
}

/**
 * åˆ›å»ºæ•°æ®åº“è¡¨
 */
async function ensureTable() {
  const client = await pool.connect()
  try {
    await client.query(`
      CREATE TABLE IF NOT EXISTS dictionary_entries (
        id SERIAL PRIMARY KEY,
        term VARCHAR(500) NOT NULL,
        term_simplified VARCHAR(500),
        definition TEXT NOT NULL,
        definition_text TEXT,
        source VARCHAR(100) NOT NULL,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        UNIQUE(term, source)
      );

      CREATE INDEX IF NOT EXISTS idx_dict_term ON dictionary_entries(term);
      CREATE INDEX IF NOT EXISTS idx_dict_term_simplified ON dictionary_entries(term_simplified);
      CREATE INDEX IF NOT EXISTS idx_dict_source ON dictionary_entries(source);
    `)
    console.log('âœ… æ•°æ®åº“è¡¨å·²å°±ç»ª')
  } finally {
    client.release()
  }
}

/**
 * å¯¼å…¥è¯æ¡åˆ°æ•°æ®åº“
 */
async function importEntries(entries: DictEntry[]) {
  const client = await pool.connect()

  // æŒ‰æ¥æºåˆ†ç»„ç»Ÿè®¡
  const sourceStats: Record<string, { total: number; imported: number; skipped: number }> = {}

  // è¿‡æ»¤æ‰å·²å­˜åœ¨çš„è¯å…¸
  const filteredEntries = entries.filter((e) => {
    if (!sourceStats[e.source]) {
      sourceStats[e.source] = { total: 0, imported: 0, skipped: 0 }
    }
    sourceStats[e.source].total++

    if (EXISTING_SOURCES.has(e.source)) {
      sourceStats[e.source].skipped++
      return false
    }
    return true
  })

  console.log(`\nğŸ“Š è¯æ¡ç»Ÿè®¡:`)
  console.log('â”€'.repeat(60))
  for (const [source, stats] of Object.entries(sourceStats).sort((a, b) => b[1].total - a[1].total)) {
    const status = EXISTING_SOURCES.has(source) ? 'â­ï¸  è·³è¿‡(å·²å­˜åœ¨)' : 'ğŸ“¥ å¾…å¯¼å…¥'
    console.log(`  ${source}: ${stats.total} æ¡ ${status}`)
  }
  console.log('â”€'.repeat(60))
  console.log(`  å¾…å¯¼å…¥: ${filteredEntries.length} æ¡`)

  if (filteredEntries.length === 0) {
    console.log('\nâš ï¸  æ²¡æœ‰æ–°è¯æ¡éœ€è¦å¯¼å…¥')
    return sourceStats
  }

  console.log(`\nğŸ’¾ å¼€å§‹å¯¼å…¥...`)

  try {
    const batchSize = 100
    let totalImported = 0

    for (let i = 0; i < filteredEntries.length; i += batchSize) {
      const batch = filteredEntries.slice(i, i + batchSize)

      for (const entry of batch) {
        try {
          await client.query(
            `
            INSERT INTO dictionary_entries (term, term_simplified, definition, definition_text, source)
            VALUES ($1, $2, $3, $4, $5)
            ON CONFLICT (term, source) DO UPDATE SET
              definition = EXCLUDED.definition,
              definition_text = EXCLUDED.definition_text,
              term_simplified = EXCLUDED.term_simplified
          `,
            [
              sanitizeString(entry.term),
              sanitizeString(entry.termSimplified),
              sanitizeString(entry.definition),
              sanitizeString(entry.definitionText),
              entry.source,
            ]
          )
          sourceStats[entry.source].imported++
          totalImported++
        } catch (error) {
          // é”™è¯¯æ—¶æ‰“å°è¯¦æƒ…
          if (sourceStats[entry.source].imported === 0 && sourceStats[entry.source].skipped === 0) {
            console.error(`   âš ï¸ å¯¼å…¥é”™è¯¯ [${entry.source}]:`, error)
            console.error(`   è¯æ¡: ${entry.term?.slice(0, 50)}`)
          }
        }
      }

      // æ˜¾ç¤ºè¿›åº¦
      if ((i + batchSize) % 5000 === 0 || i + batchSize >= filteredEntries.length) {
        console.log(`   è¿›åº¦: ${Math.min(i + batchSize, filteredEntries.length)}/${filteredEntries.length} (${totalImported} æ¡æˆåŠŸ)`)
      }
    }
  } finally {
    client.release()
  }

  return sourceStats
}

/**
 * æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
 */
async function showFinalStats() {
  const client = await pool.connect()
  try {
    const result = await client.query(`
      SELECT source, COUNT(*) as count
      FROM dictionary_entries
      GROUP BY source
      ORDER BY count DESC
    `)

    console.log('\nğŸ“Š æ•°æ®åº“è¯å…¸ç»Ÿè®¡:')
    console.log('â”€'.repeat(50))
    for (const row of result.rows) {
      console.log(`  ${row.source}: ${row.count} æ¡`)
    }

    const total = await client.query('SELECT COUNT(*) FROM dictionary_entries')
    console.log('â”€'.repeat(50))
    console.log(`  æ€»è®¡: ${total.rows[0].count} æ¡`)
  } finally {
    client.release()
  }
}

async function main() {
  const mdxPath = process.argv[2]

  if (!mdxPath) {
    console.error('ç”¨æ³•: pnpm tsx src/import-amitabha-dict.ts <mdxæ–‡ä»¶è·¯å¾„>')
    process.exit(1)
  }

  // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
  const fs = await import('fs')
  if (!fs.existsSync(mdxPath)) {
    console.error(`âŒ æ–‡ä»¶ä¸å­˜åœ¨: ${mdxPath}`)
    process.exit(1)
  }

  console.log('ğŸ” é˜¿å½Œé™€ä½›è–æ•™å¤§è¾­å…¸å¯¼å…¥å·¥å…·')
  console.log('â”€'.repeat(50))
  console.log(`ğŸ“ æ–‡ä»¶: ${mdxPath}`)
  console.log(`â­ï¸  è·³è¿‡å·²å­˜åœ¨: ${Array.from(EXISTING_SOURCES).join(', ')}`)

  // ç¡®ä¿è¡¨å­˜åœ¨
  await ensureTable()

  // è§£æ MDX
  const entries = await parseMdxWithPython(mdxPath)

  // å¯¼å…¥
  await importEntries(entries)

  // æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
  await showFinalStats()

  await pool.end()
  console.log('\nâœ¨ å®Œæˆ!')
}

main().catch(console.error)
